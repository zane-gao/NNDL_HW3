
"""CIFAR-10 å·ç§¯ç¥ç»ç½‘ç»œè®­ç»ƒä¸è°ƒå‚è„šæœ¬ã€‚"""

from __future__ import annotations

import argparse
import itertools
import json
import logging
import math
import os
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Iterable, List, Sequence, Tuple

import numpy as np
import torch
from torch import nn
from torch.optim import AdamW
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms

from data.cifar10 import load_cifar10
from models import Cifar10CNN, ModelConfig, ResNet18, WideResNet28_10

# æ£€æŸ¥ PyTorch ç‰ˆæœ¬ä»¥ä½¿ç”¨æ­£ç¡®çš„ AMP API
PYTORCH_VERSION = tuple(int(x) for x in torch.__version__.split('.')[:2])
USE_NEW_AMP_API = PYTORCH_VERSION >= (1, 13)  # PyTorch 1.13+ ä½¿ç”¨æ–° API

try:
    import yaml
    YAML_AVAILABLE = True
except ImportError:
    YAML_AVAILABLE = False
    print("è­¦å‘Š: PyYAML æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨ YAML é…ç½®æ–‡ä»¶ã€‚è¯·è¿è¡Œ: pip install pyyaml")

CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)
CIFAR10_STD = (0.2470, 0.2435, 0.2616)


def setup_logger(log_dir: str | None = None) -> logging.Logger:
    """è®¾ç½®æ—¥å¿—è®°å½•å™¨ã€‚"""
    logger = logging.getLogger("CIFAR10_Training")
    logger.setLevel(logging.INFO)
    
    # æ¸…é™¤å·²æœ‰çš„ handlers
    logger.handlers.clear()
    
    # æ§åˆ¶å°è¾“å‡º
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_format = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
    console_handler.setFormatter(console_format)
    logger.addHandler(console_handler)
    
    # æ–‡ä»¶è¾“å‡º
    if log_dir:
        os.makedirs(log_dir, exist_ok=True)
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        log_file = os.path.join(log_dir, f"train_{timestamp}.log")
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        file_format = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
        file_handler.setFormatter(file_format)
        logger.addHandler(file_handler)
        logger.info(f"æ—¥å¿—ä¿å­˜åˆ°: {log_file}")
    
    return logger


def create_model(model_name: str, model_config: ModelConfig | None = None, num_classes: int = 10) -> nn.Module:
    """æ ¹æ®æ¨¡å‹åç§°åˆ›å»ºæ¨¡å‹ã€‚"""
    if model_name == "cnn":
        if model_config is None:
            model_config = ModelConfig(num_classes=num_classes)
        return Cifar10CNN(model_config)
    elif model_name == "resnet18":
        return ResNet18(num_classes=num_classes)
    elif model_name == "wideresnet28_10":
        return WideResNet28_10(num_classes=num_classes, dropout=0.3)
    else:
        raise ValueError(f"æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {model_name}")


@dataclass
class TrainMetrics:
    """è®­ç»ƒæˆ–éªŒè¯è¿‡ç¨‹ä¸­æ”¶é›†çš„æŒ‡æ ‡ã€‚"""

    loss: float
    accuracy: float


class NumpyCifarDataset(Dataset):
    """å°† NumPy å½¢å¼çš„å›¾åƒå°è£…ä¸º PyTorch Datasetã€‚"""

    def __init__(self, images: np.ndarray, labels: np.ndarray, transform: transforms.Compose | None = None) -> None:
        self.images = images
        self.labels = labels
        self.transform = transform

    def __len__(self) -> int:  # noqa: D401
        return self.images.shape[0]

    def __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:
        image = torch.from_numpy(self.images[index]).float()
        if self.transform is not None:
            image = self.transform(image)
        label = int(self.labels[index])
        return image, label


def set_seed(seed: int | None) -> None:
    """è®¾ç½®éšæœºç§å­ç¡®ä¿å¤ç°ã€‚"""
    if seed is None:
        return
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def build_transforms(augment: bool) -> Tuple[transforms.Compose, transforms.Compose]:
    """æ„å»ºè®­ç»ƒä¸éªŒè¯é˜¶æ®µæ‰€éœ€çš„å˜æ¢ã€‚"""
    if augment:
        train_sequence = [
            transforms.RandomHorizontalFlip(),
            transforms.RandomCrop(32, padding=4),
            transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),
        ]
    else:
        train_sequence = [transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)]

    eval_sequence = [transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)]
    return transforms.Compose(train_sequence), transforms.Compose(eval_sequence)


def build_dataloaders(
    batch_size: int,
    num_workers: int,
    augment: bool,
    *,
    validation_ratio: float,
    seed: int | None,
) -> Dict[str, DataLoader]:
    """åŠ è½½ CIFAR-10 æ•°æ®å¹¶è¿”å›å¯¹åº”çš„ DataLoaderã€‚"""
    data_splits = load_cifar10(validation_ratio=validation_ratio, flatten=False, dtype=np.float32)
    train_transform, eval_transform = build_transforms(augment)

    train_images, train_labels = data_splits["train"]
    train_dataset = NumpyCifarDataset(train_images, train_labels, transform=train_transform)

    loaders: Dict[str, DataLoader] = {}
    pin_memory = torch.cuda.is_available()

    loaders["train"] = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=pin_memory,
        drop_last=False,
    )

    if "val" in data_splits:
        val_images, val_labels = data_splits["val"]
        val_dataset = NumpyCifarDataset(val_images, val_labels, transform=eval_transform)
        loaders["val"] = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=pin_memory,
        )

    test_images, test_labels = data_splits["test"]
    test_dataset = NumpyCifarDataset(test_images, test_labels, transform=eval_transform)
    loaders["test"] = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=pin_memory,
    )
    return loaders


def train_one_epoch(
    model: nn.Module,
    dataloader: DataLoader,
    optimizer: torch.optim.Optimizer,
    criterion: nn.Module,
    device: torch.device,
    *,
    scaler: torch.amp.GradScaler | torch.cuda.amp.GradScaler | None = None,
) -> TrainMetrics:
    """æ‰§è¡Œä¸€ä¸ªè®­ç»ƒè½®æ¬¡ã€‚"""
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for images, labels in dataloader:
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad(set_to_none=True)
        if scaler is not None:
            # ä½¿ç”¨æ–°æ—§ API å…¼å®¹çš„å†™æ³•
            if USE_NEW_AMP_API:
                with torch.amp.autocast(device_type=device.type):
                    outputs = model(images)
                    loss = criterion(outputs, labels)
            else:
                with torch.cuda.amp.autocast():
                    outputs = model(images)
                    loss = criterion(outputs, labels)
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

        running_loss += loss.item() * images.size(0)
        preds = outputs.argmax(dim=1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)

    return TrainMetrics(loss=running_loss / total, accuracy=correct / total)


def evaluate(model: nn.Module, dataloader: DataLoader, criterion: nn.Module, device: torch.device) -> TrainMetrics:
    """åœ¨éªŒè¯æˆ–æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹ã€‚"""
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for images, labels in dataloader:
            images = images.to(device)
            labels = labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            running_loss += loss.item() * images.size(0)
            preds = outputs.argmax(dim=1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)

    return TrainMetrics(loss=running_loss / total, accuracy=correct / total)


def save_checkpoint(
    epoch: int,
    model: nn.Module,
    optimizer: torch.optim.Optimizer,
    scheduler: torch.optim.lr_scheduler._LRScheduler | None,
    best_val_acc: float,
    checkpoint_dir: str,
    model_name: str,
    *,
    is_best: bool = False,
    extra_filename: str | None = None,
) -> None:
    """ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹ã€‚"""
    os.makedirs(checkpoint_dir, exist_ok=True)
    checkpoint = {
        "epoch": epoch,
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict(),
        "best_val_acc": best_val_acc,
        "model_name": model_name,
    }
    if scheduler is not None:
        checkpoint["scheduler_state_dict"] = scheduler.state_dict()
    
    # ä¿å­˜æœ€æ–°çš„æ£€æŸ¥ç‚¹
    checkpoint_path = os.path.join(checkpoint_dir, "checkpoint_latest.pth")
    torch.save(checkpoint, checkpoint_path)
    
    # å¯é€‰åœ°é¢å¤–ä¿å­˜ä¸€ä»½å¸¦æœ‰è‡ªå®šä¹‰æ–‡ä»¶åçš„æ£€æŸ¥ç‚¹
    if extra_filename:
        extra_path = os.path.join(checkpoint_dir, extra_filename)
        torch.save(checkpoint, extra_path)
    
    # å¦‚æœæ˜¯æœ€ä½³æ¨¡å‹ï¼Œä¹Ÿä¿å­˜ä¸€ä»½
    if is_best:
        best_path = os.path.join(checkpoint_dir, "checkpoint_best.pth")
        torch.save(checkpoint, best_path)


def load_checkpoint(
    checkpoint_path: str,
    model: nn.Module,
    optimizer: torch.optim.Optimizer | None = None,
    scheduler: torch.optim.lr_scheduler._LRScheduler | None = None,
) -> Tuple[int, float]:
    """åŠ è½½è®­ç»ƒæ£€æŸ¥ç‚¹ã€‚"""
    checkpoint = torch.load(checkpoint_path, map_location="cpu")
    model.load_state_dict(checkpoint["model_state_dict"])
    
    start_epoch = checkpoint["epoch"]
    best_val_acc = checkpoint["best_val_acc"]
    
    if optimizer is not None and "optimizer_state_dict" in checkpoint:
        optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
    
    if scheduler is not None and "scheduler_state_dict" in checkpoint:
        scheduler.load_state_dict(checkpoint["scheduler_state_dict"])
    
    return start_epoch, best_val_acc


def run_training(args: argparse.Namespace) -> None:
    """æ‰§è¡Œæ ‡å‡†è®­ç»ƒæµç¨‹ã€‚"""
    # è®¾ç½®æ—¥å¿—
    logger = setup_logger(args.log_dir)
    
    set_seed(args.seed)
    device = torch.device(args.device if args.device else ("cuda" if torch.cuda.is_available() else "cpu"))
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    logger.info(f"æ¨¡å‹ç±»å‹: {args.model}")

    loaders = build_dataloaders(
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        augment=not args.no_augment,
        validation_ratio=args.val_ratio,
        seed=args.seed,
    )

    # åˆ›å»ºæ¨¡å‹
    model_config = None  # åˆå§‹åŒ–ä¸º Noneï¼Œé¿å…ä½œç”¨åŸŸé—®é¢˜
    if args.model == "cnn":
        model_config = ModelConfig(
            num_classes=10,
            dropout=args.dropout,
            linear_dropout=args.linear_dropout,
            normalization=args.normalization,
            group_norm_groups=args.group_norm_groups,
        )
        model = create_model(args.model, model_config=model_config)
    else:
        model = create_model(args.model, num_classes=10)
    
    model = model.to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    scheduler = None
    if args.scheduler == "cosine":
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)
    elif args.scheduler == "step":
        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=max(1, args.epochs // 3), gamma=0.1)

    # åˆ›å»ºæ··åˆç²¾åº¦è®­ç»ƒçš„ GradScalerï¼ˆå…¼å®¹æ–°æ—§ APIï¼‰
    scaler = None
    if args.amp and device.type in ["cuda", "cpu"]:
        if USE_NEW_AMP_API:
            scaler = torch.amp.GradScaler(device.type)
        else:
            scaler = torch.cuda.amp.GradScaler()

    # ä»æ£€æŸ¥ç‚¹æ¢å¤
    start_epoch = 1
    best_val_acc = -math.inf
    if args.resume:
        if os.path.exists(args.resume):
            logger.info(f"ä»æ£€æŸ¥ç‚¹æ¢å¤: {args.resume}")
            start_epoch, best_val_acc = load_checkpoint(args.resume, model, optimizer, scheduler)
            start_epoch += 1  # ä»ä¸‹ä¸€ä¸ª epoch å¼€å§‹
            logger.info(f"ä» epoch {start_epoch} ç»§ç»­è®­ç»ƒï¼Œå½“å‰æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")
        else:
            logger.warning(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {args.resume}ï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")

    best_state = None
    train_history = []
    start_time = time.time()
    
    for epoch in range(start_epoch, args.epochs + 1):
        train_metrics = train_one_epoch(model, loaders["train"], optimizer, criterion, device, scaler=scaler)
        val_metrics = evaluate(model, loaders["val"], criterion, device) if "val" in loaders else None
        if scheduler is not None:
            scheduler.step()

        lr_current = optimizer.param_groups[0]["lr"]
        message = (
            f"Epoch {epoch:02d}/{args.epochs} - lr={lr_current:.4g} - "
            f"train_loss={train_metrics.loss:.4f} train_acc={train_metrics.accuracy:.4f}"
        )
        
        is_best = False
        if val_metrics is not None:
            message += f" - val_loss={val_metrics.loss:.4f} val_acc={val_metrics.accuracy:.4f}"
            if val_metrics.accuracy > best_val_acc:
                best_val_acc = val_metrics.accuracy
                best_state = model.state_dict()
                is_best = True
                message += " [æ–°æœ€ä½³]"
        
        logger.info(message)
        
        # ä¿å­˜è®­ç»ƒå†å²
        epoch_record = {
            "epoch": epoch,
            "lr": lr_current,
            "train_loss": train_metrics.loss,
            "train_acc": train_metrics.accuracy,
        }
        if val_metrics is not None:
            epoch_record["val_loss"] = val_metrics.loss
            epoch_record["val_acc"] = val_metrics.accuracy
        train_history.append(epoch_record)
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if args.checkpoint_dir:
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if epoch % args.save_freq == 0:
                save_checkpoint(
                    epoch, model, optimizer, scheduler, best_val_acc,
                    args.checkpoint_dir, args.model, is_best=False,
                    extra_filename=f"checkpoint_epoch_{epoch:04d}.pth"
                )
                logger.info(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜åˆ°: {args.checkpoint_dir}")
            
            # å¦‚æœæ˜¯æœ€ä½³æ¨¡å‹ï¼Œç«‹å³ä¿å­˜
            if is_best:
                save_checkpoint(
                    epoch, model, optimizer, scheduler, best_val_acc,
                    args.checkpoint_dir, args.model, is_best=True
                )
                logger.info(f"æœ€ä½³æ¨¡å‹å·²ä¿å­˜ (epoch {epoch}, acc={best_val_acc:.4f})")

    elapsed = time.time() - start_time
    logger.info(f"è®­ç»ƒè€—æ—¶ {elapsed / 60:.2f} åˆ†é’Ÿ")
    
    # è®­ç»ƒç»“æŸåä¿å­˜æœ€ç»ˆæ£€æŸ¥ç‚¹
    if args.checkpoint_dir:
        save_checkpoint(
            args.epochs, model, optimizer, scheduler, best_val_acc,
            args.checkpoint_dir, args.model, is_best=False
        )
        logger.info(f"æœ€ç»ˆæ£€æŸ¥ç‚¹å·²ä¿å­˜åˆ°: {args.checkpoint_dir}")

    # ä¿å­˜è®­ç»ƒå†å²
    if args.log_dir:
        history_path = os.path.join(args.log_dir, "train_history.json")
        with open(history_path, "w", encoding="utf-8") as f:
            json.dump(train_history, f, indent=2, ensure_ascii=False)
        logger.info(f"è®­ç»ƒå†å²å·²ä¿å­˜åˆ°: {history_path}")

    if best_state is not None:
        model.load_state_dict(best_state)
        logger.info(f"éªŒè¯é›†æœ€ä½³å‡†ç¡®ç‡: {best_val_acc:.4f}")

    test_metrics = evaluate(model, loaders["test"], criterion, device)
    logger.info(f"æµ‹è¯•é›†: loss={test_metrics.loss:.4f} acc={test_metrics.accuracy:.4f}")

    if args.save_path:
        save_dict = {"model": model.state_dict(), "model_name": args.model}
        if args.model == "cnn" and model_config is not None:
            save_dict["config"] = model_config.__dict__
        torch.save(save_dict, args.save_path)
        logger.info(f"æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {args.save_path}")


def _product_dict(grid: Dict[str, Sequence]) -> Iterable[Dict[str, float | str]]:
    """æ„å»ºå‚æ•°ç½‘æ ¼çš„ç¬›å¡å°”ç§¯ã€‚"""
    keys = list(grid.keys())
    values = [grid[k] for k in keys]
    for combo in itertools.product(*values):
        yield dict(zip(keys, combo, strict=True))


def build_cv_loaders(
    images: np.ndarray,
    labels: np.ndarray,
    train_indices: Sequence[int],
    val_indices: Sequence[int],
    *,
    batch_size: int,
    num_workers: int,
    augment: bool,
) -> Tuple[DataLoader, DataLoader]:
    """åŸºäºç»™å®šç´¢å¼•æ„å»ºäº¤å‰éªŒè¯ä½¿ç”¨çš„ DataLoaderã€‚"""
    train_transform, eval_transform = build_transforms(augment)
    train_dataset = NumpyCifarDataset(images[train_indices], labels[train_indices], transform=train_transform)
    val_dataset = NumpyCifarDataset(images[val_indices], labels[val_indices], transform=eval_transform)
    pin_memory = torch.cuda.is_available()
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=pin_memory,
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=pin_memory,
    )
    return train_loader, val_loader




def run_cross_validation(args: argparse.Namespace) -> None:
    """æ‰§è¡Œäº¤å‰éªŒè¯å¹¶è®°å½•ç»“æœã€‚"""
    logger = setup_logger(args.log_dir)

    set_seed(args.seed)
    device = torch.device(args.device if args.device else ("cuda" if torch.cuda.is_available() else "cpu"))
    logger.info(f"äº¤å‰éªŒè¯ä½¿ç”¨è®¾å¤‡: {device}")
    logger.info(f"äº¤å‰éªŒè¯æŠ˜æ•°: {args.kfolds}")
    logger.info(f"æ¯æŠ˜è®­ç»ƒè½®æ•°: {args.cv_epochs}")

    data = load_cifar10(validation_ratio=0.0, flatten=False, dtype=np.float32)
    train_images, train_labels = data["train"]

    rng = np.random.default_rng(args.seed)
    num_samples = train_images.shape[0]
    if args.cv_sample_size is not None and 0 < args.cv_sample_size < num_samples:
        chosen = rng.choice(num_samples, size=args.cv_sample_size, replace=False)
        train_images = train_images[chosen]
        train_labels = train_labels[chosen]
        num_samples = train_images.shape[0]
        logger.info(f"äº¤å‰éªŒè¯é‡‡æ · {num_samples} ä¸ªæ ·æœ¬ç”¨äºç½‘æ ¼æœç´¢")
    else:
        logger.info(f"ä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®: {num_samples} ä¸ªæ ·æœ¬")

    indices = np.arange(num_samples)
    rng.shuffle(indices)
    folds = np.array_split(indices, args.kfolds)

    grid = {
        "lr": args.lr_grid,
        "dropout": args.dropout_grid,
        "linear_dropout": args.linear_dropout_grid,
        "normalization": args.norm_grid,
        "weight_decay": args.weight_decay_grid,
    }

    # è®¡ç®—æ€»ç»„åˆæ•°
    total_combinations = 1
    for values in grid.values():
        total_combinations *= len(values)
    logger.info(f"=== å¼€å§‹äº¤å‰éªŒè¯ç½‘æ ¼æœç´¢ ===")
    logger.info(f"æœç´¢ç©ºé—´: {grid}")
    logger.info(f"æ€»ç»„åˆæ•°: {total_combinations}")
    logger.info(f"é¢„è®¡è®­ç»ƒæ¬¡æ•°: {total_combinations * args.kfolds}")
    logger.info("=" * 50)

    best_combo: Dict[str, float | str] | None = None
    best_score = -math.inf
    records: List[Dict[str, object]] = []
    
    combo_idx = 0
    start_time = time.time()
    
    for combo in _product_dict(grid):
        combo_idx += 1
        combo_start_time = time.time()
        logger.info(f"\n[{combo_idx}/{total_combinations}] è¯„ä¼°ç»„åˆ: {combo}")
        fold_scores: List[float] = []
        for fold_id in range(args.kfolds):
            val_indices = folds[fold_id]
            train_indices = np.concatenate([folds[i] for i in range(args.kfolds) if i != fold_id])

            train_loader, val_loader = build_cv_loaders(
                train_images,
                train_labels,
                train_indices,
                val_indices,
                batch_size=args.batch_size,
                num_workers=args.num_workers,
                augment=not args.no_augment,
            )

            model_config = ModelConfig(
                num_classes=10,
                dropout=float(combo["dropout"]),
                linear_dropout=float(combo["linear_dropout"]),
                normalization=str(combo["normalization"]),
                group_norm_groups=args.group_norm_groups,
            )
            model = Cifar10CNN(model_config).to(device)
            optimizer = AdamW(
                model.parameters(),
                lr=float(combo["lr"]),
                weight_decay=float(combo["weight_decay"]),
            )
            criterion = nn.CrossEntropyLoss()

            for epoch in range(1, args.cv_epochs + 1):
                train_one_epoch(model, train_loader, optimizer, criterion, device)
            metrics = evaluate(model, val_loader, criterion, device)
            fold_scores.append(metrics.accuracy)
            logger.info(f"  æŠ˜ {fold_id + 1}/{args.kfolds}: acc={metrics.accuracy:.4f}")
            del model
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        avg_score = float(np.mean(fold_scores))
        std_score = float(np.std(fold_scores))
        combo_elapsed = time.time() - combo_start_time
        logger.info(f"ç»„åˆç»“æœ: å¹³å‡å‡†ç¡®ç‡={avg_score:.4f} Â± {std_score:.4f}, è€—æ—¶={combo_elapsed/60:.2f}åˆ†é’Ÿ")
        
        combo_serializable = {
            key: float(value) if isinstance(value, (np.floating, float, int)) else str(value)
            for key, value in combo.items()
        }
        records.append(
            {
                "params": combo_serializable,
                "fold_scores": [float(score) for score in fold_scores],
                "mean_accuracy": avg_score,
                "std_accuracy": std_score,
                "time_minutes": combo_elapsed / 60,
            }
        )
        
        # ä¿å­˜ä¸­é—´ç»“æœï¼ˆé˜²æ­¢ç¨‹åºå´©æºƒä¸¢å¤±æ•°æ®ï¼‰
        if args.log_dir and combo_idx % 5 == 0:
            intermediate_path = Path(args.log_dir) / "cv_results_intermediate.json"
            intermediate_path.parent.mkdir(parents=True, exist_ok=True)
            with open(intermediate_path, "w", encoding="utf-8") as fp:
                json.dump({
                    "processed_combinations": combo_idx,
                    "total_combinations": total_combinations,
                    "records": records,
                }, fp, ensure_ascii=False, indent=2)
            logger.info(f"ä¸­é—´ç»“æœå·²ä¿å­˜ ({combo_idx}/{total_combinations})")
        
        if avg_score > best_score:
            best_score = avg_score
            best_combo = combo_serializable
            logger.info(f"ğŸ‰ å‘ç°æ–°çš„æœ€ä½³ç»„åˆï¼å‡†ç¡®ç‡: {best_score:.4f}")

    total_elapsed = time.time() - start_time
    logger.info("\n" + "=" * 50)
    logger.info("=== äº¤å‰éªŒè¯å®Œæˆ ===")
    logger.info(f"æ€»è€—æ—¶: {total_elapsed/60:.2f} åˆ†é’Ÿ ({total_elapsed/3600:.2f} å°æ—¶)")
    logger.info(f"æœ€ä½³ç»„åˆ: {best_combo}")
    logger.info(f"æœ€ä½³å‡†ç¡®ç‡: {best_score:.4f}")
    
    # æ˜¾ç¤º Top 5 ç»“æœ
    sorted_records = sorted(records, key=lambda x: x["mean_accuracy"], reverse=True)
    logger.info("\n=== Top 5 æœ€ä½³ç»„åˆ ===")
    for i, record in enumerate(sorted_records[:5], 1):
        logger.info(f"{i}. å‡†ç¡®ç‡={record['mean_accuracy']:.4f} Â± {record['std_accuracy']:.4f}, å‚æ•°={record['params']}")

    if args.log_dir:
        timestamp = time.strftime("cv_results_%Y%m%d_%H%M%S.json")
        output_path = Path(args.log_dir) / timestamp
        output_path.parent.mkdir(parents=True, exist_ok=True)
        payload = {
            "best_combo": best_combo,
            "best_score": float(best_score),
            "kfolds": args.kfolds,
            "cv_epochs": args.cv_epochs,
            "total_combinations": total_combinations,
            "total_time_minutes": total_elapsed / 60,
            "records": records,
            "top_5": sorted_records[:5],
        }
        with open(output_path, "w", encoding="utf-8") as fp:
            json.dump(payload, fp, ensure_ascii=False, indent=2)
        logger.info(f"\næœ€ç»ˆç»“æœå·²ä¿å­˜åˆ°: {output_path}")




def parse_args() -> argparse.Namespace:
    """è§£æå‘½ä»¤è¡Œå‚æ•°ã€‚"""
    parser = argparse.ArgumentParser(description="CIFAR-10 å·ç§¯ç¥ç»ç½‘ç»œè®­ç»ƒè„šæœ¬")
    
    # é…ç½®æ–‡ä»¶
    parser.add_argument("--config", type=str, default=None, 
                       help="é…ç½®æ–‡ä»¶è·¯å¾„ (.yaml æˆ– .json)ï¼Œå¦‚æœæŒ‡å®šåˆ™ä»é…ç½®æ–‡ä»¶åŠ è½½å‚æ•°")
    
    # æ¨¡å‹é€‰æ‹©
    parser.add_argument("--model", type=str, default="cnn", choices=["cnn", "resnet18", "wideresnet28_10"], 
                       help="é€‰æ‹©æ¨¡å‹ç±»å‹: cnn (é»˜è®¤CNN), resnet18 (ResNet-18), wideresnet28_10 (WideResNet-28-10)")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--epochs", type=int, default=30, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--cv-epochs", type=int, default=5, help="äº¤å‰éªŒè¯æ¯ä¸ªæŠ˜å†…çš„è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch-size", type=int, default=128, help="æ‰¹å¤§å°")
    parser.add_argument("--lr", type=float, default=1e-3, help="å­¦ä¹ ç‡")
    parser.add_argument("--weight-decay", type=float, default=1e-4, help="æƒé‡è¡°å‡ç³»æ•°")
    
    # CNN æ¨¡å‹ç‰¹å®šå‚æ•°
    parser.add_argument("--dropout", type=float, default=0.3, help="å·ç§¯å— Dropout æ¦‚ç‡ (ä»…ç”¨äºCNNæ¨¡å‹)")
    parser.add_argument("--linear-dropout", type=float, default=0.5, help="å…¨è¿æ¥å±‚ Dropout æ¦‚ç‡ (ä»…ç”¨äºCNNæ¨¡å‹)")
    parser.add_argument("--normalization", type=str, default="batch", choices=["batch", "layer", "group", "none"], 
                       help="å½’ä¸€åŒ–ç±»å‹ (ä»…ç”¨äºCNNæ¨¡å‹)")
    parser.add_argument("--group-norm-groups", type=int, default=8, help="GroupNorm åˆ†ç»„æ•° (ä»…ç”¨äºCNNæ¨¡å‹)")
    
    # å­¦ä¹ ç‡è°ƒåº¦
    parser.add_argument("--scheduler", type=str, default="none", choices=["none", "cosine", "step"], help="å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥")
    
    # æ•°æ®åŠ è½½
    parser.add_argument("--num-workers", type=int, default=4, help="DataLoader å·¥ä½œçº¿ç¨‹æ•°")
    parser.add_argument("--device", type=str, default=None, help="æŒ‡å®šè®­ç»ƒè®¾å¤‡ï¼Œå¦‚ cuda æˆ– cpu")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    parser.add_argument("--val-ratio", type=float, default=0.1, help="è®­ç»ƒé›†åˆ’åˆ†éªŒè¯é›†æ¯”ä¾‹")
    parser.add_argument("--no-augment", action="store_true", help="å…³é—­æ•°æ®å¢å¼º")
    
    # ä¿å­˜ä¸æ¢å¤
    parser.add_argument("--save-path", type=str, default=None, help="è®­ç»ƒç»“æŸåä¿å­˜æ¨¡å‹çš„è·¯å¾„")
    parser.add_argument("--checkpoint-dir", type=str, default="checkpoints", help="æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•")
    parser.add_argument("--resume", type=str, default=None, help="ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒçš„è·¯å¾„")
    parser.add_argument("--save-freq", type=int, default=5, help="æ¯éš”å¤šå°‘ä¸ª epoch ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹")
    parser.add_argument("--log-dir", type=str, default="logs", help="æ—¥å¿—ä¿å­˜ç›®å½•")
    
    # å…¶ä»–
    parser.add_argument("--amp", action="store_true", help="å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ")
    
    # äº¤å‰éªŒè¯å‚æ•°
    parser.add_argument("--cross-validate", action="store_true", help="å¯ç”¨äº¤å‰éªŒè¯æ¨¡å¼")
    parser.add_argument("--kfolds", type=int, default=3, help="äº¤å‰éªŒè¯æŠ˜æ•°")
    parser.add_argument("--lr-grid", type=float, nargs="+", default=[1e-3, 5e-4], help="äº¤å‰éªŒè¯å­¦ä¹ ç‡å€™é€‰")
    parser.add_argument("--dropout-grid", type=float, nargs="+", default=[0.2, 0.3], help="å·ç§¯ Dropout å€™é€‰")
    parser.add_argument("--linear-dropout-grid", type=float, nargs="+", default=[0.4, 0.5], help="å…¨è¿æ¥ Dropout å€™é€‰")
    parser.add_argument("--norm-grid", type=str, nargs="+", default=["batch", "group"], help="å½’ä¸€åŒ–ç±»å‹å€™é€‰")
    parser.add_argument("--weight-decay-grid", type=float, nargs="+", default=[1e-4, 5e-4], help="æƒé‡è¡°å‡å€™é€‰")
    parser.add_argument("--cv-sample-size", type=int, default=None, help="äº¤å‰éªŒè¯æ—¶å¯é€‰çš„æ ·æœ¬å­é›†å¤§å°")
    return parser.parse_args()


def load_config_file(config_path: str) -> Dict[str, any]:
    """ä»é…ç½®æ–‡ä»¶åŠ è½½å‚æ•°ã€‚"""
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
    
    ext = os.path.splitext(config_path)[1].lower()
    
    if ext in ['.yaml', '.yml']:
        if not YAML_AVAILABLE:
            raise ImportError("ä½¿ç”¨ YAML é…ç½®æ–‡ä»¶éœ€è¦å®‰è£… PyYAML: pip install pyyaml")
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    elif ext == '.json':
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„é…ç½®æ–‡ä»¶æ ¼å¼: {ext}ï¼Œè¯·ä½¿ç”¨ .yaml, .yml æˆ– .json")


def merge_config_and_args(config_dict: Dict[str, any], args: argparse.Namespace) -> argparse.Namespace:
    """åˆå¹¶é…ç½®æ–‡ä»¶å’Œå‘½ä»¤è¡Œå‚æ•°ï¼ˆå‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆï¼‰ã€‚"""
    # è·å–å‘½ä»¤è¡Œä¸­æ˜ç¡®è®¾ç½®çš„å‚æ•°ï¼ˆéé»˜è®¤å€¼ï¼‰
    parser = argparse.ArgumentParser()
    defaults = {}
    
    # é¦–å…ˆç”¨é…ç½®æ–‡ä»¶çš„å€¼æ›´æ–° args
    for key, value in config_dict.items():
        if hasattr(args, key):
            setattr(args, key, value)
    
    return args


def main() -> None:
    args = parse_args()
    
    # å¦‚æœæŒ‡å®šäº†é…ç½®æ–‡ä»¶ï¼Œå…ˆåŠ è½½é…ç½®æ–‡ä»¶
    if args.config:
        print(f"ä»é…ç½®æ–‡ä»¶åŠ è½½å‚æ•°: {args.config}")
        config_dict = load_config_file(args.config)
        
        # å°†é…ç½®æ–‡ä»¶çš„å€¼åˆå¹¶åˆ° args ä¸­
        # æ³¨æ„ï¼šå‘½ä»¤è¡Œæ˜¾å¼æŒ‡å®šçš„å‚æ•°åº”è¯¥è¦†ç›–é…ç½®æ–‡ä»¶
        config_model = config_dict.get('model', args.model)
        
        # ä¿å­˜å‘½ä»¤è¡Œä¸­æ˜¾å¼è®¾ç½®çš„å€¼
        cli_config_path = args.config
        
        # ç”¨é…ç½®æ–‡ä»¶æ›´æ–° args
        for key, value in config_dict.items():
            if hasattr(args, key):
                # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœå‘½ä»¤è¡Œæ²¡æœ‰æŒ‡å®š config ä»¥å¤–çš„å‚æ•°ï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶çš„å€¼
                setattr(args, key, value)
        
        # æ¢å¤ config è·¯å¾„
        args.config = cli_config_path
        print(f"ä½¿ç”¨æ¨¡å‹: {args.model}")
        print(f"è®­ç»ƒè½®æ•°: {args.epochs}")
        print(f"æ‰¹å¤§å°: {args.batch_size}")
        print(f"å­¦ä¹ ç‡: {args.lr}")
    
    if args.cross_validate:
        run_cross_validation(args)
    else:
        run_training(args)


if __name__ == "__main__":
    main()
